{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c845ac25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install transformers==4.37.0\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36b46e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import nltk\n",
    "import copy\n",
    "nltk.download('punkt')\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401b1a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "device = \"cuda:0\" # the device to load the model onto\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen1.5-0.5B-Chat\",torch_dtype=torch.bfloat16,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen1.5-0.5B-Chat\", padding_side = \"left\")\n",
    "\n",
    "# mainmodel = AutoModelForCausalLM.from_pretrained(\n",
    "#     \"Qwen/Qwen1.5-0.5B-Chat\",torch_dtype=torch.bfloat16,\n",
    "# )\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen1.5-0.5B-Chat\", padding_side = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43eb7048",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading the dataset\n",
    "dataset = load_dataset(\"HuggingFaceH4/ultrachat_200k\")\n",
    "df = dataset['train_gen'].to_pandas()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf68d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Random Sampling\n",
    "subset_size = 1500  \n",
    "\n",
    "# Use the sample() method to select a random subset\n",
    "df = df.sample(n=subset_size)\n",
    "df.reset_index(inplace = True)\n",
    "df = df[['prompt','prompt_id','messages']]\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5584600",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "## 1) Separating prompts and responses\n",
    "# Concatenate all 'content' entries in the arrays\n",
    "# contents = df['messages'].apply(lambda x: x[1]['content'])\n",
    "answers = []\n",
    "for i in range(len(df)):\n",
    "#     print(i)\n",
    "    if(len(df.at[i,'messages'])<2):\n",
    "        df.drop(i,inplace=True)\n",
    "        \n",
    "for i in range(len(df)):\n",
    "    content = df.iat[i,2][1]['content']\n",
    "    answers.append(content)\n",
    "# Create a new column in the DataFrame with the concatenated content\n",
    "df['answer'] = answers\n",
    "\n",
    "df = df[['prompt','answer']]\n",
    "\n",
    "## 2) Sorting according to prompt length to incorporate curriculum learning\n",
    "df['length_col'] = df['prompt'].apply(len)\n",
    "\n",
    "# Sort the DataFrame by the length column\n",
    "df_sorted = df.sort_values(by='length_col', ascending=True)  # Use ascending=False for descending order\n",
    "df = df_sorted[['prompt','answer']]\n",
    "display(df)\n",
    "\n",
    "## 3) Removing garbage prompts with very small lengths and hence insufficient context\n",
    "df=df[-128:]\n",
    "df.reset_index(inplace=True)\n",
    "df.drop('index',axis=1)\n",
    "df = df[['prompt','answer']]\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652b2711",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the dataloader class\n",
    "class Customdataset(Dataset):\n",
    "    def __init__(self,original_dataset):\n",
    "        self.original_dataset = original_dataset\n",
    "    def __len__(self):\n",
    "        return len(self.original_dataset)\n",
    "    def __getitem__(self,index):\n",
    "        prompt = self.original_dataset.iat[index,0]\n",
    "        response = self.original_dataset.iat[index,1]\n",
    "        return prompt,response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3c2e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initializing the dataloader\n",
    "batch_size = 4\n",
    "d_train = Customdataset(df)\n",
    "dataloader = DataLoader(d_train, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954f07b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9cd72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining the custom tokenizer\n",
    "def tokenize_and_pad(texts, tokenizer):\n",
    "    # Tokenize the batch of texts\n",
    "    #tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenized_batches = [tokenizer(batch, return_tensors=\"pt\", padding=False, truncation=True, max_length = 1024) for batch in texts]\n",
    "    max_length = max(len(text['input_ids'][0]) for text in tokenized_batches)\n",
    "    tokenized_batches = [tokenizer(batch, return_tensors=\"pt\", padding=False, truncation=True, max_length = max_length) for batch in texts]\n",
    "\n",
    "    # Pad the sequences with zeros at the end\n",
    "    for batch in tokenized_batches:\n",
    "        for key in batch.keys():\n",
    "    # Calculate the amount of padding needed\n",
    "            padding_length = max(0, max_length - len(batch[key][0]))\n",
    "            if key==\"attention_mask\":\n",
    "                pad_value = 0\n",
    "            else:    \n",
    "                pad_value = tokenizer.convert_tokens_to_ids('<|endoftext|>')  # Assuming you have a tokenizer object\n",
    "\n",
    "    # Perform left padding with the <s> token\n",
    "            if padding_length > 0:\n",
    "                padding_tensor = torch.full((batch[key].shape[0], padding_length), pad_value)\n",
    "\n",
    "                # Concatenate along the correct dimension\n",
    "                # If you want to add padding to the right (columns), use dim=1\n",
    "                batch[key] = torch.cat([ padding_tensor,batch[key]], dim=1)\n",
    "                \n",
    "        \n",
    "    return tokenized_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8fb054",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_pad_output(texts, tokenizer, desired_length):\n",
    "    device = \"cuda:0\"\n",
    "    texts = texts.to(device)\n",
    "    print(texts.device)\n",
    "    # Pad the sequences with zeros at the end\n",
    "    max_length=desired_length\n",
    "    new_text=torch.zeros((texts.shape[0],desired_length))\n",
    "    i=0\n",
    "    for batch in texts:\n",
    "    # Calculate the amount of padding needed\n",
    "        padding_length = max(0, max_length - len(batch))\n",
    "        pad_value = tokenizer.convert_tokens_to_ids('<|endoftext|>')  # Assuming you have a tokenizer object\n",
    "\n",
    "    # Perform left padding with the <s> token\n",
    "        if padding_length > 0:\n",
    "            padding_tensor = torch.full((padding_length,), pad_value).to(device)\n",
    "\n",
    "            # Concatenate along the correct dimension\n",
    "            # If you want to add padding to the right (columns), use dim=1\n",
    "            new_text[i] = torch.cat([ padding_tensor,batch], dim=0)\n",
    "            i=i+1\n",
    "    \n",
    "    return new_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe714297",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_output_attention(texts, tokenizer, desired_length):\n",
    "    device = \"cuda:0\"\n",
    "    texts = texts.to(device)\n",
    "    # Pad the sequences with zeros at the end\n",
    "    max_length=desired_length\n",
    "    new_text=torch.zeros((texts.shape[0],desired_length))\n",
    "    i=0\n",
    "    for batch in texts:\n",
    "    # Calculate the amount of padding needed\n",
    "        padding_length = max(0, max_length - len(batch))\n",
    "        pad_value = 0 # Assuming you have a tokenizer object\n",
    "\n",
    "    # Perform left padding with the <s> token\n",
    "        if padding_length > 0:\n",
    "            padding_tensor = torch.full((padding_length,), pad_value).to(device)\n",
    "\n",
    "            # Concatenate along the correct dimension\n",
    "            # If you want to add padding to the right (columns), use dim=1\n",
    "            new_text[i] = torch.cat([ padding_tensor,batch], dim=0)\n",
    "            i=i+1\n",
    "    \n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa010533",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attack(prompt_ids,attention_mask_ids,ground_truth_ids,model,batch_size,tokenizer,ground_attention):\n",
    "    epsilon=100\n",
    "    device = \"cuda:2\"\n",
    "    prompt_base=copy.deepcopy(prompt_ids).to(device)\n",
    "    prompt_adv=copy.deepcopy(prompt_ids).to(device)\n",
    "    model.to(device)\n",
    "    \n",
    "    for i in range(10):\n",
    "        print(\"i\")\n",
    "        generated_text = model.generate(input_ids = prompt_adv.long(), max_new_tokens = 2048).to(device)\n",
    "        \n",
    "        if ground_truth_ids.shape[1] > generated_text.shape[1]:\n",
    "            generated_text=tokenize_and_pad_output(generated_text, tokenizer, ground_truth_ids.shape[1]).to(device)\n",
    "        else: \n",
    "            ground_truth_ids=tokenize_and_pad_output(ground_truth_ids, tokenizer, generated_text.shape[1]).to(device)\n",
    "            ground_attention=tokenize_output_attention(ground_attention, tokenizer,generated_text.shape[1]).to(device)\n",
    "\n",
    "        generated_logits = model(input_ids=generated_text.long()).logits\n",
    "        ground_logits = model(input_ids=ground_truth_ids.long()).logits\n",
    "        ground_logits = ground_logits.argmax(dim=-1)\n",
    "        generated_logits = nn.functional.softmax(generated_logits,dim=-1)\n",
    "        \n",
    "        loss=torch.tensor(0.0,requires_grad=True)\n",
    "        \n",
    "        for number in range(batch_size):\n",
    "            for word in range(generated_logits.shape[1]):\n",
    "                loss=-torch.log(generated_logits[number][word][ground_logits[number][word]])\n",
    "                \n",
    "        fgsm = torch.sum(torch.sum(torch.autograd.grad(loss,generated_logits)[0],dim=-1),dim=-1)/generated_logits.shape[1]\n",
    "        \n",
    "        prompt_adv = torch.tensor(prompt_adv+5*fgsm.unsqueeze(1))\n",
    "        eta = prompt_adv-prompt_base\n",
    "        eta = torch.clamp(eta,-epsilon,epsilon)\n",
    "        prompt_adv = prompt_base + eta\n",
    "\n",
    "\n",
    "#     prompt_ent=entropy_projection(prompt_adv)\n",
    "    model.to(\"cuda:0\")\n",
    "    return prompt_adv.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886dac0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using BLEU Score as the evaluation metric\n",
    "def calculate_bleu_score(paragraph1, paragraph2):\n",
    "    # Remove tokens in the form of <...> from both paragraphs\n",
    "    paragraph1_clean = \" \".join(word for word in paragraph1.split() if not word.startswith(\"<\") and not word.endswith(\">\"))\n",
    "    paragraph2_clean = \" \".join(word for word in paragraph2.split() if not word.startswith(\"<\") and not word.endswith(\">\"))\n",
    "    \n",
    "    # Tokenize the paragraphs into lists of words\n",
    "    reference = nltk.word_tokenize(paragraph1_clean)\n",
    "    candidate = nltk.word_tokenize(paragraph2_clean)\n",
    "    \n",
    "    # Calculate BLEU scores\n",
    "    bleu_1 = sentence_bleu([reference], candidate, weights=(1, 0, 0, 0),smoothing_function=nltk.translate.bleu_score.SmoothingFunction().method4)\n",
    "    bleu_2 = sentence_bleu([reference], candidate, weights=(0.5, 0.5, 0, 0),smoothing_function=nltk.translate.bleu_score.SmoothingFunction().method4)\n",
    "    bleu_3 = sentence_bleu([reference], candidate, weights=(0.33, 0.33, 0.33, 0),smoothing_function=nltk.translate.bleu_score.SmoothingFunction().method4)\n",
    "    bleu_4 = sentence_bleu([reference], candidate,smoothing_function=nltk.translate.bleu_score.SmoothingFunction().method4)\n",
    "    \n",
    "    return bleu_1, bleu_2, bleu_3, bleu_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14c5ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initializing the optimizer and loading the model\n",
    "device = \"cuda:0\"\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-6)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b803cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_list = []\n",
    "\n",
    "for step,batch in enumerate(dataloader):\n",
    "    \n",
    "    print(\"Step No \"+str(step))\n",
    "    prompts, ground_truth = batch\n",
    "    \n",
    "    messages = [[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}] for prompt in prompts]\n",
    "    text = [tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=True) for message in messages]\n",
    "    tokenized_batches = tokenize_and_pad(text,tokenizer)\n",
    "    prompt_ids = torch.stack([x['input_ids'][0].to(device) for x in tokenized_batches], dim = 0)\n",
    "    prompt_attention_mask = torch.stack([x['attention_mask'][0].to(device) for x in tokenized_batches], dim = 0)\n",
    "    \n",
    "    ground_truth_messages = [[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": ground_tru}] for ground_tru in ground_truth]\n",
    "    ground_truth_text = [ tokenizer.apply_chat_template(ground_truth_message,tokenize=False,add_generation_prompt=True) for ground_truth_message in ground_truth_messages]\n",
    "\n",
    "    ground_truth_encoding = tokenize_and_pad(ground_truth_text,tokenizer)\n",
    "    ground_truth_ids = torch.stack([x['input_ids'][0].to(device) for x in ground_truth_encoding], dim = 0)\n",
    "    ground_truth_attention_mask = torch.stack([x['attention_mask'][0].to(device) for x in ground_truth_encoding], dim = 0)\n",
    "\n",
    "    adv_prompt = attack(prompt_ids, prompt_attention_mask, ground_truth_ids, model, batch_size,tokenizer,ground_truth_attention_mask).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        synthetic_response = model.generate(input_ids = adv_prompt, max_new_tokens = 2048)\n",
    "        output=[output_ids[len(input_ids):] for input_ids, output_ids in zip(tokenized_batches[0].input_ids, synthetic_response)]\n",
    "        synthetic_response_ids = torch.empty((1,output[0].size(0))).to(device)\n",
    "        for j in range(batch_size):\n",
    "            output=[output_ids[len(input_ids):] for input_ids, output_ids in zip(tokenized_batches[j].input_ids, synthetic_response)]\n",
    "            synthetic_response_ids=torch.cat([synthetic_response_ids.long(),output[0].unsqueeze(0)],dim = 0)\n",
    "        synthetic_response_ids=synthetic_response_ids[1:,:]\n",
    "                \n",
    "    for i in range(len(synthetic_response_ids)):\n",
    "        scores = calculate_bleu_score(tokenizer.decode(synthetic_response_ids[i]),ground_truth_text[i])\n",
    "        score_list.append(scores)\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    model.to(device)\n",
    "            \n",
    "avg_bleu_score = sum(score[0] for score in score_list)/len(score_list)\n",
    "print(f\"Average BLEU-1 Score is {avg_bleu_score}\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e3adf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
