{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fb8d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install transformers==4.37.0\n",
    "!pip install nltk\n",
    "!pip install apricot-select\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7d9976",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import nltk\n",
    "import itertools \n",
    "import copy\n",
    "nltk.download('punkt')\n",
    "import apricot\n",
    "from scipy.sparse import csr_matrix\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228dbc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "device = \"cuda:0\" # the device to load the model onto\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen1.5-0.5B-Chat\",torch_dtype=torch.bfloat16,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen1.5-0.5B-Chat\", padding_side = \"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c02708",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading the dataset\n",
    "dataset = load_dataset(\"HuggingFaceH4/ultrachat_200k\")\n",
    "df = dataset['train_gen'].to_pandas()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb9901e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Random Sampling\n",
    "subset_size = 1500  \n",
    "\n",
    "# Use the sample() method to select a random subset\n",
    "df = df.sample(n=subset_size)\n",
    "df.reset_index(inplace = True)\n",
    "df = df[['prompt','prompt_id','messages']]\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54712758",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "## 1) Separating prompts and responses\n",
    "# Concatenate all 'content' entries in the arrays\n",
    "# contents = df['messages'].apply(lambda x: x[1]['content'])\n",
    "answers = []\n",
    "for i in range(len(df)):\n",
    "#     print(i)\n",
    "    if(len(df.at[i,'messages'])<2):\n",
    "        df.drop(i,inplace=True)\n",
    "        \n",
    "for i in range(len(df)):\n",
    "    content = df.iat[i,2][1]['content']\n",
    "    answers.append(content)\n",
    "# Create a new column in the DataFrame with the concatenated content\n",
    "df['answer'] = answers\n",
    "\n",
    "df = df[['prompt','answer']]\n",
    "\n",
    "## 2) Sorting according to prompt length to incorporate curriculum learning\n",
    "df['length_col'] = df['prompt'].apply(len)\n",
    "\n",
    "# Sort the DataFrame by the length column\n",
    "df_sorted = df.sort_values(by='length_col', ascending=True)  # Use ascending=False for descending order\n",
    "df = df_sorted[['prompt','answer']]\n",
    "display(df)\n",
    "\n",
    "## 3) Removing garbage prompts with very small lengths and hence insufficient context\n",
    "df=df[-1024:]\n",
    "df.reset_index(inplace=True)\n",
    "df.drop('index',axis=1)\n",
    "df = df[['prompt','answer']]\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d169c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the dataloader class\n",
    "class Customdataset(Dataset):\n",
    "    def __init__(self,original_dataset):\n",
    "        self.original_dataset = original_dataset\n",
    "    def __len__(self):\n",
    "        return len(self.original_dataset)\n",
    "    def __getitem__(self,index):\n",
    "        prompt = self.original_dataset.iat[index,0]\n",
    "        response = self.original_dataset.iat[index,1]\n",
    "        return prompt,response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bbf738",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initializing the dataloader\n",
    "batch_size = 4\n",
    "d_train = Customdataset(df)\n",
    "dataloader = DataLoader(d_train, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66085ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)\n",
    "# torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ec33f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining the custom tokenizer\n",
    "def tokenize_and_pad(texts, tokenizer):\n",
    "    # Tokenize the batch of texts\n",
    "    #tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenized_batches = [tokenizer(batch, return_tensors=\"pt\", padding=False, truncation=True, max_length = 1024) for batch in texts]\n",
    "    max_length = max(len(text['input_ids'][0]) for text in tokenized_batches)\n",
    "    tokenized_batches = [tokenizer(batch, return_tensors=\"pt\", padding=False, truncation=True, max_length = max_length) for batch in texts]\n",
    "\n",
    "    # Pad the sequences with zeros at the end\n",
    "    for batch in tokenized_batches:\n",
    "        for key in batch.keys():\n",
    "    # Calculate the amount of padding needed\n",
    "            padding_length = max(0, max_length - len(batch[key][0]))\n",
    "            if key==\"attention_mask\":\n",
    "                pad_value = 0\n",
    "            else:    \n",
    "                pad_value = tokenizer.convert_tokens_to_ids('<|endoftext|>')  # Assuming you have a tokenizer object\n",
    "\n",
    "    # Perform left padding with the <s> token\n",
    "            if padding_length > 0:\n",
    "                padding_tensor = torch.full((batch[key].shape[0], padding_length), pad_value)\n",
    "\n",
    "                # Concatenate along the correct dimension\n",
    "                # If you want to add padding to the right (columns), use dim=1\n",
    "                batch[key] = torch.cat([ padding_tensor,batch[key]], dim=1)\n",
    "                \n",
    "        \n",
    "    return tokenized_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3aee67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_pad_output(texts, tokenizer, desired_length):\n",
    "    device = \"cuda:0\"\n",
    "    texts = texts.to(device)\n",
    "    print(texts.device)\n",
    "    # Pad the sequences with zeros at the end\n",
    "    max_length=desired_length\n",
    "    new_text=torch.zeros((texts.shape[0],desired_length))\n",
    "    i=0\n",
    "    for batch in texts:\n",
    "    # Calculate the amount of padding needed\n",
    "        padding_length = max(0, max_length - len(batch))\n",
    "        pad_value = tokenizer.convert_tokens_to_ids('<|endoftext|>')  # Assuming you have a tokenizer object\n",
    "\n",
    "    # Perform left padding with the <s> token\n",
    "        if padding_length > 0:\n",
    "            padding_tensor = torch.full((padding_length,), pad_value).to(device)\n",
    "\n",
    "            # Concatenate along the correct dimension\n",
    "            # If you want to add padding to the right (columns), use dim=1\n",
    "            new_text[i] = torch.cat([ padding_tensor,batch], dim=0)\n",
    "            i=i+1\n",
    "    \n",
    "    return new_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1590b0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_output_attention(texts, tokenizer, desired_length):\n",
    "    device = \"cuda:0\"\n",
    "    texts = texts.to(device)\n",
    "    # Pad the sequences with zeros at the end\n",
    "    max_length=desired_length\n",
    "    new_text=torch.zeros((texts.shape[0],desired_length))\n",
    "    i=0\n",
    "    for batch in texts:\n",
    "    # Calculate the amount of padding needed\n",
    "        padding_length = max(0, max_length - len(batch))\n",
    "        pad_value = 0 # Assuming you have a tokenizer object\n",
    "\n",
    "    # Perform left padding with the <s> token\n",
    "        if padding_length > 0:\n",
    "            padding_tensor = torch.full((padding_length,), pad_value).to(device)\n",
    "\n",
    "            # Concatenate along the correct dimension\n",
    "            # If you want to add padding to the right (columns), use dim=1\n",
    "            new_text[i] = torch.cat([ padding_tensor,batch], dim=0)\n",
    "            i=i+1\n",
    "    \n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4e17ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attack(prompt_ids,attention_mask_ids,ground_truth_ids,model,batch_size,tokenizer,ground_attention):\n",
    "    epsilon=100\n",
    "    device = \"cuda:2\"\n",
    "    prompt_base=copy.deepcopy(prompt_ids).to(device)\n",
    "    prompt_adv=copy.deepcopy(prompt_ids).to(device)\n",
    "    model.to(device)\n",
    "    \n",
    "    for i in range(10):\n",
    "        print(\"i\")\n",
    "        generated_text = model.generate(input_ids = prompt_adv.long(), max_new_tokens = 2048).to(device)\n",
    "        \n",
    "        if ground_truth_ids.shape[1] > generated_text.shape[1]:\n",
    "            generated_text=tokenize_and_pad_output(generated_text, tokenizer, ground_truth_ids.shape[1]).to(device)\n",
    "        else: \n",
    "            ground_truth_ids=tokenize_and_pad_output(ground_truth_ids, tokenizer, generated_text.shape[1]).to(device)\n",
    "            ground_attention=tokenize_output_attention(ground_attention, tokenizer,generated_text.shape[1]).to(device)\n",
    "\n",
    "        generated_logits = model(input_ids=generated_text.long()).logits\n",
    "        ground_logits = model(input_ids=ground_truth_ids.long()).logits\n",
    "        ground_logits = ground_logits.argmax(dim=-1)\n",
    "        generated_logits = nn.functional.softmax(generated_logits,dim=-1)\n",
    "        \n",
    "        loss=torch.tensor(0.0,requires_grad=True)\n",
    "        \n",
    "        for number in range(batch_size):\n",
    "            for word in range(generated_logits.shape[1]):\n",
    "                loss=-torch.log(generated_logits[number][word][ground_logits[number][word]])\n",
    "                \n",
    "        fgsm = torch.sum(torch.sum(torch.autograd.grad(loss,generated_logits)[0],dim=-1),dim=-1)/generated_logits.shape[1]\n",
    "        \n",
    "        prompt_adv = torch.tensor(prompt_adv+5*fgsm.unsqueeze(1))\n",
    "        eta = prompt_adv-prompt_base\n",
    "        eta = torch.clamp(eta,-epsilon,epsilon)\n",
    "        prompt_adv = prompt_base + eta\n",
    "\n",
    "\n",
    "#     prompt_ent=entropy_projection(prompt_adv)\n",
    "    model.to(\"cuda:0\")\n",
    "    return prompt_adv.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741d65b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_indices(model, tokenizer, d_train, batch_size, optimizer):\n",
    "\n",
    "    subset_loader = torch.utils.data.DataLoader(d_train, batch_size = batch_size, shuffle=False)\n",
    "    opponent_logits_gt_list = []\n",
    "    grads = []\n",
    "    k=0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(subset_loader):\n",
    "        prompts, ground_truth = batch\n",
    "        messages = [[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}] for prompt in prompts]\n",
    "        text = [tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=True) for message in messages]\n",
    "        tokenized_batches = tokenize_and_pad(text,tokenizer)\n",
    "        prompt_ids = torch.stack([x['input_ids'][0].to(device) for x in tokenized_batches], dim = 0)\n",
    "        prompt_attention_mask = torch.stack([x['attention_mask'][0].to(device) for x in tokenized_batches], dim = 0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            synthetic_response = model.generate(input_ids = prompt_ids, max_new_tokens = 2048)\n",
    "            output = [output_ids[len(input_ids):] for input_ids, output_ids in zip(tokenized_batches[0].input_ids, synthetic_response[0].unsqueeze(0))]\n",
    "            synthetic_response_ids = torch.empty((1,output[0].size(0))).to(device)\n",
    "            \n",
    "            for j in range(batch_size):\n",
    "                output=[output_ids[len(input_ids):] for input_ids, output_ids in zip(tokenized_batches[j].input_ids, synthetic_response[j].unsqueeze(0))]\n",
    "                synthetic_response_ids=torch.cat([synthetic_response_ids.long(),output[0].unsqueeze(0)],dim = 0)\n",
    "            synthetic_response_ids=synthetic_response_ids[1:,:]\n",
    "\n",
    "            # Calculate opponent's logits for ground truth responses\n",
    "            ground_truth_messages = [[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": ground_tru}] for ground_tru in ground_truth]\n",
    "            ground_truth_text = [ tokenizer.apply_chat_template(ground_truth_message,tokenize=False,add_generation_prompt=True) for ground_truth_message in ground_truth_messages]\n",
    "    \n",
    "            ground_truth_encoding = tokenize_and_pad(ground_truth_text,tokenizer)\n",
    "            ground_truth_ids = torch.stack([x['input_ids'][0].to(device) for x in ground_truth_encoding], dim = 0)\n",
    "            ground_truth_attention_mask = torch.stack([x['attention_mask'][0].to(device) for x in ground_truth_encoding], dim = 0)\n",
    "            \n",
    "            opponent_logits_gt = model(\n",
    "                input_ids=ground_truth_ids, \n",
    "                attention_mask=ground_truth_attention_mask\n",
    "            ).logits\n",
    "            opponent_logits_gt_list.append(opponent_logits_gt)\n",
    "    \n",
    "        # Train the main player model using the synthetic data and real responses\n",
    "        model.train()  # Set model to training model\n",
    "        \n",
    "        # Calculate logits for ground truth and synthetic responses using the main player model\n",
    "        main_player_logits_gt = model(input_ids=ground_truth_ids, attention_mask=ground_truth_attention_mask).logits\n",
    "        main_player_logits_syn = model(input_ids=synthetic_response_ids).logits\n",
    "        \n",
    "        # Compute logits for synthetic responses using the opponent model (disabled adapter layers)\n",
    "        opponent_logits_syn = model(input_ids=synthetic_response_ids).logits\n",
    "        \n",
    "\n",
    "        # Compute the loss\n",
    "        loss = compute_spin_loss(\n",
    "            main_player_logits_gt, opponent_logits_gt,\n",
    "            main_player_logits_syn, opponent_logits_syn,\n",
    "            ground_truth_ids, synthetic_response_ids, lambda_reg=0.1\n",
    "        )\n",
    "        \n",
    "        l0_grads = torch.autograd.grad(loss, opponent_logits_syn)[0]\n",
    "        grads.append(l0_grads)\n",
    "        dist_max,maximum=compute_distance(grads)\n",
    "        \n",
    "        for i, j in itertools.combinations(range(len(grads)), 2):\n",
    "            dist_max[i][j] = maximum-dist_max[i][j]\n",
    "            dist_max[j][i] = maximum-dist_max[i][j]\n",
    "            \n",
    "        print(\"LOOP DONE\")\n",
    "\n",
    "    fl = apricot.functions.facilityLocation.FacilityLocationSelection(random_state=0,metric='precomputed',n_samples=32,optimizer='stochastic')\n",
    "    sim_sub = fl.fit_transform(np.array(dist_max))\n",
    "    greedyList = list(np.argmax(sim_sub, axis=1))\n",
    "    return greedyList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0cc266",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distance(grads):\n",
    "    maximum=0\n",
    "    dist_max = [[0 for i in range(len(grads))] for j in range(len(grads))]\n",
    "    for i, j in itertools.combinations(range(len(grads)), 2):\n",
    "        min_length = min(grads[i].shape[1], grads[j].shape[1])\n",
    "        \n",
    "        dist = float(torch.sum(torch.square(grads[i][:,:min_length, :] - grads[j][:,:min_length,:]))) # Euclidean distance\n",
    "        dist_max[i][j] = dist\n",
    "        dist_max[j][i] = dist\n",
    "        maximum=max(dist,maximum)\n",
    "    \n",
    "    return dist_max,maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72bb897",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining the SPIN-finetuning loss \n",
    "def compute_spin_loss(model_logits_gt, opponent_logits_gt, model_logits_syn, opponent_logits_syn, ground_truth_ids, synthetic_response_ids, lambda_reg=0.1):\n",
    "    # Apply softmax to convert logits to probabilities\n",
    "    # Shapes after softmax: [batch_size, sequence_length, vocab_size]\n",
    "    model_probs_gt = torch.nn.functional.softmax(model_logits_gt, dim=-1)\n",
    "    opponent_probs_gt = torch.nn.functional.softmax(opponent_logits_gt, dim=-1)\n",
    "    model_probs_syn = torch.nn.functional.softmax(model_logits_syn, dim=-1)\n",
    "    opponent_probs_syn = torch.nn.functional.softmax(opponent_logits_syn, dim=-1)\n",
    "\n",
    "    # Gather log probabilities for the actual tokens in the ground truth sequence\n",
    "    # [batch_size, sequence_length, vocab_size] -> [batch_size, sequence_length]\n",
    "    log_model_probs_gt = torch.log(torch.gather(\n",
    "        model_probs_gt, dim=2, index=ground_truth_ids.unsqueeze(-1)\n",
    "    ).squeeze(-1))\n",
    "    log_opponent_probs_gt = torch.log(torch.gather(\n",
    "        opponent_probs_gt, dim=2, index=ground_truth_ids.unsqueeze(-1)\n",
    "    ).squeeze(-1))\n",
    "\n",
    "    # Gather log probabilities for the actual tokens in the synthetic sequence\n",
    "    # [batch_size, sequence_length, vocab_size] -> [batch_size, sequence_length]\n",
    "    log_model_probs_syn = torch.log(torch.gather(\n",
    "        model_probs_syn, dim=2, index=synthetic_response_ids.unsqueeze(-1)\n",
    "    ).squeeze(-1))\n",
    "    log_opponent_probs_syn = torch.log(torch.gather(\n",
    "        opponent_probs_syn, dim=2, index=synthetic_response_ids.unsqueeze(-1)\n",
    "    ).squeeze(-1))\n",
    "\n",
    "    # Calculate log probability ratios for the tokens in the sequence\n",
    "    # [batch_size, sequence_length]\n",
    "    log_prob_ratio_gt = log_model_probs_gt - log_opponent_probs_gt\n",
    "    log_prob_ratio_syn = log_model_probs_syn - log_opponent_probs_syn\n",
    "\n",
    "    # Sum the log probability ratios over the sequence\n",
    "    # [batch_size] -> scalar\n",
    "    sum_log_prob_ratio_gt = torch.sum(log_prob_ratio_gt, dim=1)\n",
    "    sum_log_prob_ratio_syn = torch.sum(log_prob_ratio_syn, dim=1)\n",
    "\n",
    "    # Calculate the combined loss term for each sequence in the batch, scaled by lambda_reg\n",
    "    # [batch_size] -> scalar\n",
    "    combined_loss = lambda_reg * (sum_log_prob_ratio_gt - sum_log_prob_ratio_syn)\n",
    "\n",
    "    # Apply the logistic loss to the combined term\n",
    "    # [batch_size] -> scalar\n",
    "    logistic_loss = torch.log(1 + torch.exp(-combined_loss))\n",
    "\n",
    "    # Compute the mean of the logistic loss across the batch\n",
    "    # scalar\n",
    "    spin_loss = logistic_loss.mean()\n",
    "    return spin_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f183cb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using BLEU Score as the evaluation metric\n",
    "def calculate_bleu_score(paragraph1, paragraph2):\n",
    "    # Remove tokens in the form of <...> from both paragraphs\n",
    "    paragraph1_clean = \" \".join(word for word in paragraph1.split() if not word.startswith(\"<\") and not word.endswith(\">\"))\n",
    "    paragraph2_clean = \" \".join(word for word in paragraph2.split() if not word.startswith(\"<\") and not word.endswith(\">\"))\n",
    "    \n",
    "    # Tokenize the paragraphs into lists of words\n",
    "    reference = nltk.word_tokenize(paragraph1_clean)\n",
    "    candidate = nltk.word_tokenize(paragraph2_clean)\n",
    "    \n",
    "    # Calculate BLEU scores\n",
    "    bleu_1 = sentence_bleu([reference], candidate, weights=(1, 0, 0, 0),smoothing_function=nltk.translate.bleu_score.SmoothingFunction().method4)\n",
    "    bleu_2 = sentence_bleu([reference], candidate, weights=(0.5, 0.5, 0, 0),smoothing_function=nltk.translate.bleu_score.SmoothingFunction().method4)\n",
    "    bleu_3 = sentence_bleu([reference], candidate, weights=(0.33, 0.33, 0.33, 0),smoothing_function=nltk.translate.bleu_score.SmoothingFunction().method4)\n",
    "    bleu_4 = sentence_bleu([reference], candidate,smoothing_function=nltk.translate.bleu_score.SmoothingFunction().method4)\n",
    "    \n",
    "    return bleu_1, bleu_2, bleu_3, bleu_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea71b028",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-6)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0aeb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iters = 3\n",
    "\n",
    "for iter in range(num_iters):\n",
    "    print(\"Training Epoch\"+str(iter+1)+\"/\"+str(num_iters))\n",
    "    total_loss = 0\n",
    "    losses = []\n",
    "    synthetic_data = []\n",
    "    opponent_logits_gt_list = []\n",
    "    \n",
    "    greedyList = compute_indices(model, tokenizer, d_train, batch_size, optimizer)\n",
    "\n",
    "    for step,batch in enumerate(dataloader):\n",
    "        print(\"Step No \"+str(step))\n",
    "        if step not in greedyList:\n",
    "            continue\n",
    "            \n",
    "        prompts, ground_truth = batch\n",
    "        messages = [[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}] for prompt in prompts]\n",
    "        text = [tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=True) for message in messages]\n",
    "        tokenized_batches = tokenize_and_pad(text,tokenizer)\n",
    "        prompt_ids = torch.stack([x['input_ids'][0].to(device) for x in tokenized_batches], dim = 0)\n",
    "        prompt_attention_mask = torch.stack([x['attention_mask'][0].to(device) for x in tokenized_batches], dim = 0)\n",
    "\n",
    "        ground_truth_messages = [[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": ground_tru}] for ground_tru in ground_truth]\n",
    "        ground_truth_text = [ tokenizer.apply_chat_template(ground_truth_message,tokenize=False,add_generation_prompt=True) for ground_truth_message in ground_truth_messages]\n",
    "\n",
    "        ground_truth_encoding = tokenize_and_pad(ground_truth_text,tokenizer)\n",
    "        ground_truth_ids = torch.stack([x['input_ids'][0].to(device) for x in ground_truth_encoding], dim = 0)\n",
    "        ground_truth_attention_mask = torch.stack([x['attention_mask'][0].to(device) for x in ground_truth_encoding], dim = 0)\n",
    "\n",
    "        prompt_ids = attack(prompt_ids, prompt_attention_mask, ground_truth_ids, model, batch_size,tokenizer,ground_truth_attention_mask).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            synthetic_response = model.generate(input_ids = prompt_ids, max_new_tokens = 2048)\n",
    "        \n",
    "            output=[output_ids[len(input_ids):] for input_ids, output_ids in zip(tokenized_batches[0].input_ids, synthetic_response)]\n",
    "            synthetic_response_ids = torch.empty((1,output[0].size(0))).to(device)\n",
    "            for j in range(batch_size):\n",
    "                output=[output_ids[len(input_ids):] for input_ids, output_ids in zip(tokenized_batches[j].input_ids, synthetic_response)]\n",
    "                synthetic_response_ids=torch.cat([synthetic_response_ids.long(),output[0].unsqueeze(0)],dim = 0)\n",
    "            synthetic_response_ids=synthetic_response_ids[1:,:]\n",
    "            \n",
    "            opponent_logits_gt = model(\n",
    "                input_ids=ground_truth_ids, \n",
    "                attention_mask=ground_truth_attention_mask\n",
    "            ).logits\n",
    "\n",
    "            opponent_logits_gt_list.append(opponent_logits_gt)\n",
    "            \n",
    "            opponent_logits_syn = model(input_ids=synthetic_response_ids).logits\n",
    "            \n",
    "        model.train()\n",
    "\n",
    "        main_player_logits_gt = model(input_ids=ground_truth_ids, attention_mask=ground_truth_attention_mask).logits\n",
    "        main_player_logits_syn = model(input_ids=synthetic_response_ids).logits\n",
    "        \n",
    "#         Compute the loss\n",
    "        loss = compute_spin_loss(\n",
    "            main_player_logits_gt, opponent_logits_gt,\n",
    "            main_player_logits_syn, opponent_logits_syn,\n",
    "            ground_truth_ids, synthetic_response_ids, lambda_reg=0.1\n",
    "        )\n",
    "#         loss = torch.tensor([1.0]).to(device)\n",
    "        total_loss += loss.item()\n",
    "        losses.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "#         torch.cuda.empty_cache()\n",
    "#         gc.collect()    \n",
    "#         model.to(device)\n",
    "\n",
    "    average_loss = total_loss/(len(dataloader))\n",
    "    print(f\"Iteration {iter + 1}/{num_iters}, Average Loss: {average_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5deac3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8fb717",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "d_test = Customdataset(df)\n",
    "test_dataloader = DataLoader(d_test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe51e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_list = []\n",
    "\n",
    "for step,batch in enumerate(dataloader):\n",
    "    \n",
    "    print(\"Step No \"+str(step))\n",
    "    prompts, ground_truth = batch\n",
    "    \n",
    "    messages = [[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}] for prompt in prompts]\n",
    "    text = [tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=True) for message in messages]\n",
    "    tokenized_batches = tokenize_and_pad(text,tokenizer)\n",
    "    prompt_ids = torch.stack([x['input_ids'][0].to(device) for x in tokenized_batches], dim = 0)\n",
    "    prompt_attention_mask = torch.stack([x['attention_mask'][0].to(device) for x in tokenized_batches], dim = 0)\n",
    "    \n",
    "    ground_truth_messages = [[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": ground_tru}] for ground_tru in ground_truth]\n",
    "    ground_truth_text = [ tokenizer.apply_chat_template(ground_truth_message,tokenize=False,add_generation_prompt=True) for ground_truth_message in ground_truth_messages]\n",
    "\n",
    "    ground_truth_encoding = tokenize_and_pad(ground_truth_text,tokenizer)\n",
    "    ground_truth_ids = torch.stack([x['input_ids'][0].to(device) for x in ground_truth_encoding], dim = 0)\n",
    "    ground_truth_attention_mask = torch.stack([x['attention_mask'][0].to(device) for x in ground_truth_encoding], dim = 0)\n",
    "\n",
    "    adv_prompt = attack(prompt_ids, prompt_attention_mask, ground_truth_ids, model, batch_size,tokenizer,ground_truth_attention_mask).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        synthetic_response = model.generate(input_ids = adv_prompt, max_new_tokens = 2048)\n",
    "        output=[output_ids[len(input_ids):] for input_ids, output_ids in zip(tokenized_batches[0].input_ids, synthetic_response)]\n",
    "        synthetic_response_ids = torch.empty((1,output[0].size(0))).to(device)\n",
    "        for j in range(batch_size):\n",
    "            output=[output_ids[len(input_ids):] for input_ids, output_ids in zip(tokenized_batches[j].input_ids, synthetic_response)]\n",
    "            synthetic_response_ids=torch.cat([synthetic_response_ids.long(),output[0].unsqueeze(0)],dim = 0)\n",
    "        synthetic_response_ids=synthetic_response_ids[1:,:]\n",
    "                \n",
    "    for i in range(len(synthetic_response_ids)):\n",
    "        scores = calculate_bleu_score(tokenizer.decode(synthetic_response_ids[i]),ground_truth_text[i])\n",
    "        score_list.append(scores)\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    model.to(device)\n",
    "            \n",
    "avg_bleu_score = sum(score[0] for score in score_list)/len(score_list)\n",
    "print(f\"Average BLEU-1 Score is {avg_bleu_score}\")\n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
