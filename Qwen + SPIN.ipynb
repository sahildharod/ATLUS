{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcac35d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install transformers==4.37.0\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51703fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import nltk\n",
    "import copy\n",
    "nltk.download('punkt')\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5745bf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen1.5-0.5B-Chat\",torch_dtype=torch.bfloat16,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen1.5-0.5B-Chat\", padding_side = \"left\")\n",
    "\n",
    "# mainmodel = AutoModelForCausalLM.from_pretrained(\n",
    "#     \"Qwen/Qwen1.5-0.5B-Chat\",torch_dtype=torch.bfloat16,\n",
    "# )\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen1.5-0.5B-Chat\", padding_side = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca98c647",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:2\"\n",
    "model.to(device)\n",
    "print(model)\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a619bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mainmodel.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579373a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading the dataset\n",
    "dataset = load_dataset(\"HuggingFaceH4/ultrachat_200k\")\n",
    "df = dataset['train_gen'].to_pandas()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc372ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Random Sampling\n",
    "subset_size = 256  \n",
    "\n",
    "# Use the sample() method to select a random subset\n",
    "df = df.sample(n=subset_size)\n",
    "df.reset_index(inplace = True)\n",
    "df = df[['prompt','prompt_id','messages']]\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81345430",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "## 1) Separating prompts and responses\n",
    "# Concatenate all 'content' entries in the arrays\n",
    "# contents = df['messages'].apply(lambda x: x[1]['content'])\n",
    "answers = []\n",
    "for i in range(len(df)):\n",
    "#     print(i)\n",
    "    if(len(df.at[i,'messages'])<2):\n",
    "        df.drop(i,inplace=True)\n",
    "        \n",
    "for i in range(len(df)):\n",
    "    content = df.iat[i,2][1]['content']\n",
    "    answers.append(content)\n",
    "# Create a new column in the DataFrame with the concatenated content\n",
    "df['answer'] = answers\n",
    "\n",
    "df = df[['prompt','answer']]\n",
    "\n",
    "## 2) Sorting according to prompt length to incorporate curriculum learning\n",
    "df['length_col'] = df['prompt'].apply(len)\n",
    "\n",
    "# Sort the DataFrame by the length column\n",
    "df_sorted = df.sort_values(by='length_col', ascending=True)  # Use ascending=False for descending order\n",
    "df = df_sorted[['prompt','answer']]\n",
    "display(df)\n",
    "\n",
    "## 3) Removing garbage prompts with very small lengths and hence insufficient context\n",
    "df=df[10:138]\n",
    "df.reset_index(inplace=True)\n",
    "df.drop('index',axis=1)\n",
    "df = df[['prompt','answer']]\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b429c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the dataloader class\n",
    "class Customdataset(Dataset):\n",
    "    def __init__(self,original_dataset):\n",
    "        self.original_dataset = original_dataset\n",
    "    def __len__(self):\n",
    "        return len(self.original_dataset)\n",
    "    def __getitem__(self,index):\n",
    "        prompt = self.original_dataset.iat[index,0]\n",
    "        response = self.original_dataset.iat[index,1]\n",
    "        return prompt,response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9f312c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initializing the dataloader\n",
    "batch_size = 2\n",
    "d_train = Customdataset(df)\n",
    "dataloader = DataLoader(d_train, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363dabbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)\n",
    "# torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f5d642",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining the custom tokenizer\n",
    "def tokenize_and_pad(texts, tokenizer):\n",
    "    # Tokenize the batch of texts\n",
    "    #tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenized_batches = [tokenizer(batch, return_tensors=\"pt\", padding=False, truncation=True, max_length = 1024) for batch in texts]\n",
    "    max_length = max(len(text['input_ids'][0]) for text in tokenized_batches)\n",
    "    tokenized_batches = [tokenizer(batch, return_tensors=\"pt\", padding=False, truncation=True, max_length = max_length) for batch in texts]\n",
    "\n",
    "    # Pad the sequences with zeros at the end\n",
    "    for batch in tokenized_batches:\n",
    "        for key in batch.keys():\n",
    "    # Calculate the amount of padding needed\n",
    "            padding_length = max(0, max_length - len(batch[key][0]))\n",
    "            if key==\"attention_mask\":\n",
    "                pad_value = 0\n",
    "            else:    \n",
    "                pad_value = tokenizer.convert_tokens_to_ids('<|endoftext|>')  # Assuming you have a tokenizer object\n",
    "\n",
    "    # Perform left padding with the <s> token\n",
    "            if padding_length > 0:\n",
    "                padding_tensor = torch.full((batch[key].shape[0], padding_length), pad_value)\n",
    "\n",
    "                # Concatenate along the correct dimension\n",
    "                # If you want to add padding to the right (columns), use dim=1\n",
    "                batch[key] = torch.cat([ padding_tensor,batch[key]], dim=1)\n",
    "                \n",
    "        \n",
    "    return tokenized_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6920583a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using BLEU Score as the evaluation metric\n",
    "def calculate_bleu_score(paragraph1, paragraph2):\n",
    "    # Remove tokens in the form of <...> from both paragraphs\n",
    "    paragraph1_clean = \" \".join(word for word in paragraph1.split() if not word.startswith(\"<\") and not word.endswith(\">\"))\n",
    "    paragraph2_clean = \" \".join(word for word in paragraph2.split() if not word.startswith(\"<\") and not word.endswith(\">\"))\n",
    "    \n",
    "    # Tokenize the paragraphs into lists of words\n",
    "    reference = nltk.word_tokenize(paragraph1_clean)\n",
    "    candidate = nltk.word_tokenize(paragraph2_clean)\n",
    "    \n",
    "    # Calculate BLEU scores\n",
    "    bleu_1 = sentence_bleu([reference], candidate, weights=(1, 0, 0, 0),smoothing_function=nltk.translate.bleu_score.SmoothingFunction().method4)\n",
    "    bleu_2 = sentence_bleu([reference], candidate, weights=(0.5, 0.5, 0, 0),smoothing_function=nltk.translate.bleu_score.SmoothingFunction().method4)\n",
    "    bleu_3 = sentence_bleu([reference], candidate, weights=(0.33, 0.33, 0.33, 0),smoothing_function=nltk.translate.bleu_score.SmoothingFunction().method4)\n",
    "    bleu_4 = sentence_bleu([reference], candidate,smoothing_function=nltk.translate.bleu_score.SmoothingFunction().method4)\n",
    "    \n",
    "    return bleu_1, bleu_2, bleu_3, bleu_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32e6dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initializing the optimizer and loading the model\n",
    "# device = \"cuda:2\"\n",
    "# model.to(device)\n",
    "# print(model)\n",
    "# optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c557af50",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining the SPIN-finetuning loss \n",
    "def compute_spin_loss(model_logits_gt, opponent_logits_gt, model_logits_syn, opponent_logits_syn, ground_truth_ids, synthetic_response_ids, lambda_reg=0.1):\n",
    "    # Apply softmax to convert logits to probabilities\n",
    "    # Shapes after softmax: [batch_size, sequence_length, vocab_size]\n",
    "    model_probs_gt = torch.nn.functional.softmax(model_logits_gt, dim=-1)\n",
    "    opponent_probs_gt = torch.nn.functional.softmax(opponent_logits_gt, dim=-1)\n",
    "    model_probs_syn = torch.nn.functional.softmax(model_logits_syn, dim=-1)\n",
    "    opponent_probs_syn = torch.nn.functional.softmax(opponent_logits_syn, dim=-1)\n",
    "\n",
    "    # Gather log probabilities for the actual tokens in the ground truth sequence\n",
    "    # [batch_size, sequence_length, vocab_size] -> [batch_size, sequence_length]\n",
    "    log_model_probs_gt = torch.log(torch.gather(\n",
    "        model_probs_gt, dim=2, index=ground_truth_ids.unsqueeze(-1)\n",
    "    ).squeeze(-1))\n",
    "    log_opponent_probs_gt = torch.log(torch.gather(\n",
    "        opponent_probs_gt, dim=2, index=ground_truth_ids.unsqueeze(-1)\n",
    "    ).squeeze(-1))\n",
    "\n",
    "    # Gather log probabilities for the actual tokens in the synthetic sequence\n",
    "    # [batch_size, sequence_length, vocab_size] -> [batch_size, sequence_length]\n",
    "    log_model_probs_syn = torch.log(torch.gather(\n",
    "        model_probs_syn, dim=2, index=synthetic_response_ids.unsqueeze(-1)\n",
    "    ).squeeze(-1))\n",
    "    log_opponent_probs_syn = torch.log(torch.gather(\n",
    "        opponent_probs_syn, dim=2, index=synthetic_response_ids.unsqueeze(-1)\n",
    "    ).squeeze(-1))\n",
    "\n",
    "    # Calculate log probability ratios for the tokens in the sequence\n",
    "    # [batch_size, sequence_length]\n",
    "    log_prob_ratio_gt = log_model_probs_gt - log_opponent_probs_gt\n",
    "    log_prob_ratio_syn = log_model_probs_syn - log_opponent_probs_syn\n",
    "\n",
    "    # Sum the log probability ratios over the sequence\n",
    "    # [batch_size] -> scalar\n",
    "    sum_log_prob_ratio_gt = torch.sum(log_prob_ratio_gt, dim=1)\n",
    "    sum_log_prob_ratio_syn = torch.sum(log_prob_ratio_syn, dim=1)\n",
    "\n",
    "    # Calculate the combined loss term for each sequence in the batch, scaled by lambda_reg\n",
    "    # [batch_size] -> scalar\n",
    "    combined_loss = lambda_reg * (sum_log_prob_ratio_gt - sum_log_prob_ratio_syn)\n",
    "\n",
    "    # Apply the logistic loss to the combined term\n",
    "    # [batch_size] -> scalar\n",
    "    logistic_loss = torch.log(1 + torch.exp(-combined_loss))\n",
    "\n",
    "    # Compute the mean of the logistic loss across the batch\n",
    "    # scalar\n",
    "    spin_loss = logistic_loss.mean()\n",
    "    return spin_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053c7a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iters = 3\n",
    "\n",
    "for iter in range(num_iters):\n",
    "    print(\"Training Epoch\"+str(iter+1)+\"/\"+str(num_iters))\n",
    "    total_loss = 0\n",
    "    losses = []\n",
    "    synthetic_data = []\n",
    "    opponent_logits_gt_list = []\n",
    "\n",
    "    for step,batch in enumerate(dataloader):\n",
    "        print(\"Step No \"+str(step))\n",
    "\n",
    "        prompts, ground_truth = batch\n",
    "        messages = [[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}] for prompt in prompts]\n",
    "        text = [tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=True) for message in messages]\n",
    "        tokenized_batches = tokenize_and_pad(text,tokenizer)\n",
    "        prompt_ids = torch.stack([x['input_ids'][0].to(device) for x in tokenized_batches], dim = 0)\n",
    "        prompt_attention_mask = torch.stack([x['attention_mask'][0].to(device) for x in tokenized_batches], dim = 0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            synthetic_response = model.generate(input_ids = prompt_ids, max_new_tokens = 2048)\n",
    "        \n",
    "            output=[output_ids[len(input_ids):] for input_ids, output_ids in zip(tokenized_batches[0].input_ids, synthetic_response)]\n",
    "            synthetic_response_ids = torch.empty((1,output[0].size(0))).to(device)\n",
    "            for j in range(batch_size):\n",
    "                output=[output_ids[len(input_ids):] for input_ids, output_ids in zip(tokenized_batches[j].input_ids, synthetic_response)]\n",
    "                synthetic_response_ids=torch.cat([synthetic_response_ids.long(),output[0].unsqueeze(0)],dim = 0)\n",
    "            synthetic_response_ids=synthetic_response_ids[1:,:]\n",
    "            \n",
    "            ground_truth_messages = [[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": ground_tru}] for ground_tru in ground_truth]\n",
    "            ground_truth_text = [ tokenizer.apply_chat_template(ground_truth_message,tokenize=False,add_generation_prompt=True) for ground_truth_message in ground_truth_messages]\n",
    "\n",
    "            ground_truth_encoding = tokenize_and_pad(ground_truth_text,tokenizer)\n",
    "            ground_truth_ids = torch.stack([x['input_ids'][0].to(device) for x in ground_truth_encoding], dim = 0)\n",
    "            ground_truth_attention_mask = torch.stack([x['attention_mask'][0].to(device) for x in ground_truth_encoding], dim = 0)\n",
    "\n",
    "            opponent_logits_gt = model(\n",
    "                input_ids=ground_truth_ids, \n",
    "                attention_mask=ground_truth_attention_mask\n",
    "            ).logits\n",
    "\n",
    "            opponent_logits_gt_list.append(opponent_logits_gt)\n",
    "            \n",
    "            opponent_logits_syn = model(input_ids=synthetic_response_ids).logits\n",
    "        model.train()\n",
    "\n",
    "\n",
    "        main_player_logits_gt = model(input_ids=ground_truth_ids, attention_mask=ground_truth_attention_mask).logits\n",
    "        main_player_logits_syn = model(input_ids=synthetic_response_ids).logits\n",
    "        \n",
    "#         Compute the loss\n",
    "        loss = compute_spin_loss(\n",
    "            main_player_logits_gt, opponent_logits_gt,\n",
    "            main_player_logits_syn, opponent_logits_syn,\n",
    "            ground_truth_ids, synthetic_response_ids, lambda_reg=0.1\n",
    "        )\n",
    "#         loss = torch.tensor([1.0]).to(device)\n",
    "        total_loss += loss.item()\n",
    "        losses.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "#         torch.cuda.empty_cache()\n",
    "#         gc.collect()    \n",
    "#         model.to(device)\n",
    "\n",
    "    average_loss = total_loss/(len(dataloader))\n",
    "    print(f\"Iteration {iter + 1}/{num_iters}, Average Loss: {average_loss}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64956ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505b74fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd2869e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "d_test = Customdataset(df)\n",
    "test_dataloader = DataLoader(d_test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0a277f",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_list = []\n",
    "\n",
    "for step,batch in enumerate(dataloader):\n",
    "    \n",
    "    print(\"Step No \"+str(step))\n",
    "    prompts, ground_truth = batch\n",
    "    messages = [[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}] for prompt in prompts]\n",
    "    text = [tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=True) for message in messages]\n",
    "    tokenized_batches = tokenize_and_pad(text,tokenizer)\n",
    "    prompt_ids = torch.stack([x['input_ids'][0].to(device) for x in tokenized_batches], dim = 0)\n",
    "    prompt_attention_mask = torch.stack([x['attention_mask'][0].to(device) for x in tokenized_batches], dim = 0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        synthetic_response = model.generate(input_ids = prompt_ids, max_new_tokens = 2048)\n",
    "        output=[output_ids[len(input_ids):] for input_ids, output_ids in zip(tokenized_batches[0].input_ids, synthetic_response)]\n",
    "        synthetic_response_ids = torch.empty((1,output[0].size(0))).to(device)\n",
    "        for j in range(batch_size):\n",
    "            output=[output_ids[len(input_ids):] for input_ids, output_ids in zip(tokenized_batches[j].input_ids, synthetic_response)]\n",
    "            synthetic_response_ids=torch.cat([synthetic_response_ids.long(),output[0].unsqueeze(0)],dim = 0)\n",
    "        synthetic_response_ids=synthetic_response_ids[1:,:]\n",
    "        print(tokenizer.decode(synthetic_response_ids[0]))\n",
    "        \n",
    "        ground_truth_messages = [[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": ground_tru}] for ground_tru in ground_truth]\n",
    "        ground_truth_text = [ tokenizer.apply_chat_template(ground_truth_message,tokenize=False,add_generation_prompt=True) for ground_truth_message in ground_truth_messages]\n",
    "        \n",
    "    for i in range(len(synthetic_response_ids)):\n",
    "        scores = calculate_bleu_score(tokenizer.decode(synthetic_response_ids[i]),ground_truth_text[i])\n",
    "        score_list.append(scores)\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    model.to(device)\n",
    "            \n",
    "avg_bleu_score = sum(score[0] for score in score_list)/len(score_list)\n",
    "print(f\"Average BLEU-1 Score is {avg_bleu_score}\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca119451",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(synthetic_response[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e2bd84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
